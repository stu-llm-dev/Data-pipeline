{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mynew\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math, unicodedata, torch\n",
    "from typing import List, Tuple, Optional, Union, Dict, Any, Iterable, Callable\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json, csv\n",
    "import os\n",
    "import re, html\n",
    "try:\n",
    "    import markdown as mdlib\n",
    "    from bs4 import BeautifulSoup\n",
    "    _HAVE_MD_STACK = True\n",
    "except Exception:\n",
    "    _HAVE_MD_STACK = False\n",
    "\n",
    "# -------- utils --------\n",
    "def md_to_plain_fallback(s: str, strip_code_blocks: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    fallback อย่างง่าย: ตัด markdown เบื้องต้นด้วย regex\n",
    "    \"\"\"\n",
    "    text = s\n",
    "    if strip_code_blocks:\n",
    "        text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)\n",
    "        text = re.sub(r\"`[^`]*`\", \"\", text)\n",
    "    else:\n",
    "        # เก็บ code blocks ไว้แต่ลอก tag อื่น ๆ ออก\n",
    "        pass\n",
    "    text = re.sub(r\"\\!\\[[^\\]]*\\]\\([^)]*\\)\", \"\", text)      # images\n",
    "    text = re.sub(r\"\\[[^\\]]*\\]\\([^)]*\\)\", r\"\\1\", text)     # links -> anchor text\n",
    "    text = re.sub(r\"[#*_>~-]+\", \" \", text)                 # md symbols\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def is_markdown_path(path: str) -> bool:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    return ext in (\".md\", \".markdown\")\n",
    "\n",
    "def md_to_plain_external(s: str, strip_code_blocks: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    ใช้ markdown → HTML แล้วใช้ BeautifulSoup ดึงข้อความล้วน\n",
    "    - strip_code_blocks=True จะลบ <pre>/<code> ออก\n",
    "    - False จะคงโค้ดบล็อกไว้ โดยห่อด้วย ``` และ inline code ด้วย backticks\n",
    "    \"\"\"\n",
    "    if not _HAVE_MD_STACK:\n",
    "        return md_to_plain_fallback(s, strip_code_blocks)\n",
    "\n",
    "    html_doc = mdlib.markdown(\n",
    "        s,\n",
    "        extensions=[\n",
    "            \"extra\",          # รวม tables, abbr, etc.\n",
    "            \"fenced_code\",\n",
    "            \"sane_lists\",\n",
    "            \"codehilite\",\n",
    "            \"toc\",\n",
    "            \"smarty\",\n",
    "        ],\n",
    "        output_format=\"html5\",\n",
    "    )\n",
    "    soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "\n",
    "    # จัดการรูปภาพ: ใช้ alt เป็นข้อความ\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        alt = img.get(\"alt\") or \"\"\n",
    "        img.replace_with(alt)\n",
    "\n",
    "    if strip_code_blocks:\n",
    "        # ลบ code blocks ทั้งหมด\n",
    "        for tag in soup.find_all([\"pre\", \"code\"]):\n",
    "            tag.decompose()\n",
    "    else:\n",
    "        # เก็บ code blocks: แทน <pre> ด้วย ``` ... ```\n",
    "        for pre in soup.find_all(\"pre\"):\n",
    "            code_text = pre.get_text(\"\\n\")\n",
    "            pre.replace_with(\"\\n```\\n\" + code_text + \"\\n```\\n\")\n",
    "        # inline code: <code>...</code> → `...`\n",
    "        for code in soup.find_all(\"code\"):\n",
    "            if code.parent and code.parent.name != \"pre\":\n",
    "                code.replace_with(\"`\" + code.get_text() + \"`\")\n",
    "\n",
    "    # ลิงก์: ใช้ข้อความลิงก์ (ตัว soup.get_text จะเก็บ text ของ <a> ให้อยู่แล้ว)\n",
    "    text = soup.get_text(\"\\n\")\n",
    "\n",
    "    # ทำความสะอาดเล็กน้อย\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_file_format(path: str) -> str:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".md\", \".markdown\"):\n",
    "        return \"md\"\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    if ext in (\".jsonl\", \".ndjson\"):\n",
    "        return \"jsonl\"\n",
    "    return \"text\"  # txt/อื่น ๆ\n",
    "\n",
    "def autodetect_text_field(keys: List[str]) -> Optional[str]:\n",
    "    # เดาจากคีย์ที่พบบ่อย\n",
    "    candidates = [\"ข้อความ\", \"text\", \"Text\", \"message\", \"content\", \"utterance\"]\n",
    "    for c in candidates:\n",
    "        if c in keys:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def read_jsonl_texts(\n",
    "    path: str,\n",
    "    field: Optional[str] = None,\n",
    "    max_rows: Optional[int] = None,\n",
    "    skip_empty: bool = True,\n",
    "    encoding: str = \"utf-8\"\n",
    ") -> Tuple[List[str], Dict[str, Any]]:\n",
    "    texts: List[str] = []\n",
    "    num_lines = 0\n",
    "    used_field = field\n",
    "    with open(path, \"r\", encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if used_field is None:\n",
    "                used_field = autodetect_text_field(list(obj.keys()))\n",
    "            val = obj.get(used_field) if used_field else None\n",
    "            if val is None:\n",
    "                # ถ้าไม่มีฟิลด์ชัดเจน ลองเลือกฟิลด์แรกที่เป็นสตริง\n",
    "                for k, v in obj.items():\n",
    "                    if isinstance(v, str):\n",
    "                        val = v\n",
    "                        used_field = k\n",
    "                        break\n",
    "            if isinstance(val, str):\n",
    "                s = val.strip()\n",
    "                if (not skip_empty) or s:\n",
    "                    texts.append(s)\n",
    "                    num_lines += 1\n",
    "            if max_rows is not None and num_lines >= max_rows:\n",
    "                break\n",
    "    meta = {\"detected_text_field\": used_field, \"num_rows\": num_lines}\n",
    "    return texts, meta\n",
    "\n",
    "def read_csv_texts(\n",
    "    path: str,\n",
    "    text_col: Optional[str] = None,\n",
    "    sep: Optional[str] = None,\n",
    "    encoding: str = \"utf-8-sig\",\n",
    "    max_rows: Optional[int] = None,\n",
    "    skip_empty: bool = True,\n",
    ") -> Tuple[List[str], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    อ่าน CSV แบบไม่พึ่ง pandas (ถ้าอยากใช้ pandas ก็แทนส่วนนี้ได้)\n",
    "    - เดา delimiter ถ้า sep=None\n",
    "    - เดาชื่อคอลัมน์ถ้า text_col=None\n",
    "    \"\"\"\n",
    "    texts: List[str] = []\n",
    "    used_sep = sep\n",
    "    used_col = text_col\n",
    "    num_rows = 0\n",
    "\n",
    "    with open(path, \"r\", encoding=encoding, newline=\"\") as f:\n",
    "        sample = f.read(4096)\n",
    "        f.seek(0)\n",
    "        if used_sep is None:\n",
    "            try:\n",
    "                used_sep = csv.Sniffer().sniff(sample).delimiter\n",
    "            except Exception:\n",
    "                used_sep = \",\"  # fallback\n",
    "        reader = csv.DictReader(f, delimiter=used_sep)\n",
    "        if reader.fieldnames is None:\n",
    "            return [], {\"detected_text_col\": None, \"num_rows\": 0, \"delimiter\": used_sep}\n",
    "\n",
    "        if used_col is None:\n",
    "            used_col = autodetect_text_field(reader.fieldnames)\n",
    "\n",
    "        for row in reader:\n",
    "            val = row.get(used_col) if used_col else None\n",
    "            if isinstance(val, str):\n",
    "                s = val.strip()\n",
    "                if (not skip_empty) or s:\n",
    "                    texts.append(s)\n",
    "                    num_rows += 1\n",
    "            if max_rows is not None and num_rows >= max_rows:\n",
    "                break\n",
    "\n",
    "    meta = {\"detected_text_col\": used_col, \"num_rows\": num_rows, \"delimiter\": used_sep}\n",
    "    return texts, meta\n",
    "\n",
    "def sanitize_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = s.replace(\"\\u00A0\", \" \").replace(\"\\u200B\", \"\").replace(\"\\u200C\", \"\").replace(\"\\u200D\", \"\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def get_all_special_ids(tokenizer) -> set:\n",
    "    \"\"\"รวบรวม special token IDs ทั้งหมด\"\"\"\n",
    "    ids = set()\n",
    "    for tid in [tokenizer.eos_token_id, tokenizer.bos_token_id, tokenizer.pad_token_id]:\n",
    "        if tid is not None:\n",
    "            ids.add(tid)\n",
    "    # เพิ่มจาก all_special_ids ถ้ามี\n",
    "    if hasattr(tokenizer, 'all_special_ids') and tokenizer.all_special_ids:\n",
    "        ids.update(tokenizer.all_special_ids)\n",
    "    return ids\n",
    "\n",
    "def mask_special_tokens(labels: torch.Tensor, special_ids: set) -> torch.Tensor:\n",
    "    \"\"\"Mask special tokens ให้เป็น -100\"\"\"\n",
    "    if not special_ids:\n",
    "        return labels\n",
    "    masked = labels.clone()\n",
    "    for tid in special_ids:\n",
    "        masked[masked == tid] = -100\n",
    "    return masked\n",
    "\n",
    "def supports_chat_template(tokenizer) -> bool:\n",
    "    \"\"\"ตรวจสอบว่า tokenizer รองรับ chat template หรือไม่\"\"\"\n",
    "    return hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None\n",
    "\n",
    "def batch_iterator(items: List[Any], batch_size: int) -> Iterable[List[Any]]:\n",
    "    \"\"\"แบ่งรายการเป็น batches\"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i:i + batch_size]\n",
    "\n",
    "def encode_with_step(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    max_length: int,\n",
    "    step_tokens: int,\n",
    "    use_chat_template: bool,\n",
    "    eval_role: str = \"assistant\",\n",
    "    assistant_user_prompt: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    "    chunk_head: int = 3,\n",
    "    chunk_tail: int = 1,\n",
    ") -> List[List[Dict[str, torch.Tensor]]]:\n",
    "    encoded_per_line = []\n",
    "\n",
    "    for idx, text in enumerate(texts):\n",
    "        if len(text.strip()) == 0:\n",
    "            encoded_per_line.append([])\n",
    "            continue\n",
    "\n",
    "        # Apply chat template (ตามเดิม)\n",
    "        if use_chat_template:\n",
    "            if eval_role == \"assistant\":\n",
    "                user_msg = assistant_user_prompt or \"โปรดพิมพ์ข้อความต่อไปนี้ซ้ำตามตัวอักษรโดยไม่แก้ไข\"\n",
    "                conv = [\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                    {\"role\": \"assistant\", \"content\": text}\n",
    "                ]\n",
    "            else:\n",
    "                conv = [{\"role\": \"user\", \"content\": text}]\n",
    "            try:\n",
    "                rendered = tokenizer.apply_chat_template(conv, add_generation_prompt=False, tokenize=False)\n",
    "            except:\n",
    "                rendered = text\n",
    "        else:\n",
    "            rendered = text\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Text {idx+1}: {len(text)} chars -> {len(rendered)} chars (after template)\")\n",
    "\n",
    "        # เข้ารหัสเต็มก่อน\n",
    "        full_enc = tokenizer(rendered, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        total_tokens = full_enc[\"input_ids\"].size(1)\n",
    "        if verbose:\n",
    "            print(f\"  Total tokens: {total_tokens}\")\n",
    "\n",
    "        if total_tokens <= max_length:\n",
    "            chunks = [{\n",
    "                \"input_ids\": full_enc[\"input_ids\"],\n",
    "                \"attention_mask\": full_enc[\"attention_mask\"],\n",
    "            }]\n",
    "            if verbose:\n",
    "                print(f\"  No sliding needed: 1 chunk of {total_tokens} tokens\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  Using manual sliding window...\")\n",
    "            full_enc_list = tokenizer(rendered, add_special_tokens=True)\n",
    "            input_ids_list = full_enc_list[\"input_ids\"]\n",
    "\n",
    "            chunks = []\n",
    "            debug_chunks = []  # (tok_count, start_idx, end_idx)\n",
    "            start_idx = 0\n",
    "            while start_idx < len(input_ids_list):\n",
    "                end_idx = min(start_idx + max_length, len(input_ids_list))\n",
    "                chunk_input_ids = input_ids_list[start_idx:end_idx]\n",
    "                chunk_attention = [1] * len(chunk_input_ids)\n",
    "                chunk = {\n",
    "                    \"input_ids\": torch.tensor([chunk_input_ids], dtype=torch.long),\n",
    "                    \"attention_mask\": torch.tensor([chunk_attention], dtype=torch.long),\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                debug_chunks.append((len(chunk_input_ids), start_idx, end_idx))\n",
    "                if end_idx >= len(input_ids_list):\n",
    "                    break\n",
    "                start_idx += step_tokens\n",
    "\n",
    "            if verbose:\n",
    "                total_chunks = len(debug_chunks)\n",
    "                print(f\"  Total chunks: {total_chunks}\")\n",
    "                # พิมพ์เฉพาะตัวอย่างหัว/ท้าย\n",
    "                if total_chunks <= (chunk_head + chunk_tail):\n",
    "                    for i, (tok, st, ed) in enumerate(debug_chunks, 1):\n",
    "                        print(f\"    Chunk {i}: {tok} tokens (pos {st}:{ed})\")\n",
    "                else:\n",
    "                    # head\n",
    "                    for i, (tok, st, ed) in enumerate(debug_chunks[:chunk_head], 1):\n",
    "                        print(f\"    Chunk {i}: {tok} tokens (pos {st}:{ed})\")\n",
    "                    # ellipsis\n",
    "                    skipped = total_chunks - (chunk_head + chunk_tail)\n",
    "                    print(f\"    ... {skipped} more chunks ...\")\n",
    "                    # tail\n",
    "                    start_num = total_chunks - chunk_tail + 1\n",
    "                    for j, (tok, st, ed) in enumerate(debug_chunks[-chunk_tail:], start_num):\n",
    "                        print(f\"    Chunk {j}: {tok} tokens (pos {st}:{ed})\")\n",
    "        encoded_per_line.append(chunks)\n",
    "    return encoded_per_line\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_ppl_per_line(\n",
    "    model_name: str,\n",
    "    texts: List[str],\n",
    "    batch_size: int = 4,\n",
    "    max_length: int = 1024,\n",
    "    step_tokens: Optional[int] = None,\n",
    "    use_chat_template: Optional[bool] = None,\n",
    "    eval_role: str = \"assistant\",\n",
    "    assistant_user_prompt: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[List[float], List[int]]:\n",
    "    \"\"\"\n",
    "    คำนวณ PPL สำหรับแต่ละบรรทัด/ข้อความ\n",
    "    คืน: (ppl_list, token_counts_list)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load model with offline support\n",
    "    is_local_path = os.path.exists(model_name) and os.path.isdir(model_name)\n",
    "    load_kwargs = {\"use_fast\": True}\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": (torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    "        \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
    "    }\n",
    "\n",
    "    if is_local_path:\n",
    "        load_kwargs[\"local_files_only\"] = True\n",
    "        model_kwargs[\"local_files_only\"] = True\n",
    "        if verbose:\n",
    "            print(f\"Loading local model from: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, **load_kwargs)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs).eval()\n",
    "    except OSError as e:\n",
    "        if \"couldn't connect\" in str(e) or \"Network is unreachable\" in str(e):\n",
    "            if verbose:\n",
    "                print(\"Network unavailable, trying offline mode...\")\n",
    "            load_kwargs[\"local_files_only\"] = True\n",
    "            model_kwargs[\"local_files_only\"] = True\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, **load_kwargs)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs).eval()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Determine chat template usage\n",
    "    template_supported = supports_chat_template(tokenizer)\n",
    "    if use_chat_template is None:\n",
    "        model_name_lower = model_name.lower()\n",
    "        likely_instruct = any(keyword in model_name_lower for keyword in [\n",
    "            'instruct', 'chat', 'it', 'sft', 'alpaca', 'vicuna', 'llama-2-chat', 'llama-3-instruct', 'gemma'\n",
    "        ])\n",
    "        will_use_template = template_supported and likely_instruct\n",
    "    else:\n",
    "        will_use_template = use_chat_template and template_supported\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Chat template supported: {template_supported}\")\n",
    "        print(f\"Will use chat template: {will_use_template}\")\n",
    "        print(f\"Processing {len(texts)} texts...\")\n",
    "\n",
    "    # Get special tokens\n",
    "    special_ids = get_all_special_ids(tokenizer)\n",
    "\n",
    "    # Sanitize texts\n",
    "    clean_texts = [sanitize_text(t) for t in texts]\n",
    "\n",
    "    if step_tokens is None or step_tokens < 1:\n",
    "        # เผื่อผู้ใช้ลืมคำนวณ step มาก่อน: ใช้ค่า default = 25% ของ max_length\n",
    "        step_tokens = max(1, max_length // 4)\n",
    "\n",
    "    # Encode with sliding window\n",
    "    encoded_per_line = encode_with_step(\n",
    "        tokenizer, clean_texts, max_length, step_tokens, will_use_template,\n",
    "        eval_role, assistant_user_prompt,\n",
    "        verbose=verbose,\n",
    "        chunk_head=3,\n",
    "        chunk_tail=1,\n",
    "    )\n",
    "\n",
    "    ppl_list: List[float] = []\n",
    "    tok_list: List[int] = []\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    # Process each text (line)\n",
    "    for line_idx, chunks in enumerate(encoded_per_line):\n",
    "        if not chunks:\n",
    "            ppl_list.append(float('inf'))\n",
    "            tok_list.append(0)\n",
    "            continue\n",
    "\n",
    "        sum_nll = 0.0\n",
    "        sum_tokens = 0\n",
    "\n",
    "        # Process chunks in batches\n",
    "        for chunk_batch in batch_iterator(chunks, batch_size):\n",
    "            # Combine into single batch\n",
    "            input_ids = torch.cat([c[\"input_ids\"] for c in chunk_batch], dim=0).to(device)\n",
    "            attention_mask = torch.cat([c[\"attention_mask\"] for c in chunk_batch], dim=0).to(device)\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = input_ids.clone()\n",
    "            labels = mask_special_tokens(labels, special_ids)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=torch.cuda.is_available()):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            # Shift for causal LM\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            shift_mask = ((shift_labels != -100) & (attention_mask[:, 1:] == 1)).to(torch.float32)\n",
    "\n",
    "            # Calculate loss\n",
    "            vocab_size = shift_logits.size(-1)\n",
    "            token_loss = loss_fct(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "            token_loss = token_loss.view(shift_labels.size(0), shift_labels.size(1))\n",
    "\n",
    "            # Apply mask and sum\n",
    "            masked_loss = token_loss * shift_mask\n",
    "            nll_per_seq = masked_loss.sum(dim=1)\n",
    "            tokens_per_seq = shift_mask.sum(dim=1)\n",
    "\n",
    "            sum_nll += float(nll_per_seq.sum().item())\n",
    "            sum_tokens += int(tokens_per_seq.sum().item())\n",
    "\n",
    "        # Calculate PPL for this line\n",
    "        if sum_tokens > 0:\n",
    "            ppl = math.exp(sum_nll / sum_tokens)\n",
    "        else:\n",
    "            ppl = float('inf')\n",
    "\n",
    "        ppl_list.append(ppl)\n",
    "        tok_list.append(sum_tokens)\n",
    "\n",
    "        if verbose and (line_idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {line_idx + 1}/{len(texts)} texts\")\n",
    "\n",
    "    return ppl_list, tok_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_ppl(\n",
    "    model_name: str,\n",
    "    text_or_path: Union[str, List[str]],\n",
    "    context_length: int = 1024,\n",
    "    overlap_ratio: Optional[float] = 0.25,   # กำหนด overlap เป็นสัดส่วน\n",
    "    overlap: Optional[int] = None,           # หรือกำหนดเป็นจำนวนโทเค็นตรง ๆ\n",
    "    batch_size: int = 4,\n",
    "    use_chat_template: Optional[bool] = None,\n",
    "    eval_role: str = \"assistant\",\n",
    "    assistant_user_prompt: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    "    is_file: bool = True,\n",
    "    md_handling: str = \"auto\",\n",
    "    md_strip_code_blocks: bool = True,\n",
    "    file_format: str = \"auto\",          # \"auto\" | \"text\" | \"md\" | \"csv\" | \"jsonl\"\n",
    "    csv_text_col: Optional[str] = None,\n",
    "    csv_sep: Optional[str] = None,\n",
    "    csv_encoding: str = \"utf-8-sig\",\n",
    "    jsonl_text_field: Optional[str] = None,\n",
    "    jsonl_encoding: str = \"utf-8\",\n",
    "    max_rows: Optional[int] = None,\n",
    "    skip_empty: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function - รองรับทั้งไฟล์เดียวและรายการข้อความ\n",
    "    \"\"\"\n",
    "\n",
    "    def clamp01(x: float) -> float:\n",
    "        return 0.0 if x < 0.0 else (1.0 if x > 1.0 else x)\n",
    "\n",
    "    # Prepare texts\n",
    "    if isinstance(text_or_path, list):\n",
    "        texts = text_or_path\n",
    "        if verbose:\n",
    "            print(f\"Processing {len(texts)} texts from list\")\n",
    "    elif is_file:\n",
    "        if verbose:\n",
    "            print(f\"Reading file: {text_or_path}\")\n",
    "        if not os.path.exists(text_or_path):\n",
    "            raise FileNotFoundError(f\"File not found: {text_or_path}\")\n",
    "\n",
    "        # ตัดสินใจชนิดไฟล์\n",
    "        fmt = file_format\n",
    "        if fmt == \"auto\":\n",
    "            fmt = detect_file_format(text_or_path)\n",
    "\n",
    "        if fmt == \"csv\":\n",
    "            # อ่าน CSV เป็นรายการสตริง\n",
    "            texts, meta = read_csv_texts(\n",
    "                text_or_path,\n",
    "                text_col=csv_text_col,\n",
    "                sep=csv_sep,\n",
    "                encoding=csv_encoding,\n",
    "                max_rows=max_rows,\n",
    "                skip_empty=skip_empty,\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"CSV loaded: rows={meta.get('num_rows', 0)} | \"\n",
    "                      f\"col={meta.get('detected_text_col')} | sep={meta.get('delimiter')}\")\n",
    "        elif fmt == \"jsonl\":\n",
    "            # อ่าน JSONL เป็นรายการสตริง\n",
    "            texts, meta = read_jsonl_texts(\n",
    "                text_or_path,\n",
    "                field=jsonl_text_field,\n",
    "                max_rows=max_rows,\n",
    "                skip_empty=skip_empty,\n",
    "                encoding=jsonl_encoding,\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"JSONL loaded: rows={meta.get('num_rows', 0)} | \"\n",
    "                      f\"field={meta.get('detected_text_field')}\")\n",
    "        else:\n",
    "            # อ่านเป็นสตริงเดียว (text/md)\n",
    "            with open(text_or_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Markdown?\n",
    "            will_md = (\n",
    "                (md_handling == \"force\") or\n",
    "                (md_handling == \"auto\" and (fmt == \"md\"))\n",
    "            )\n",
    "            if will_md:\n",
    "                if verbose:\n",
    "                    print(f\"Detected Markdown → converting to plain text using \"\n",
    "                          f\"{'markdown+bs4' if _HAVE_MD_STACK else 'regex fallback'} ...\")\n",
    "                    before_len = len(content)\n",
    "                content = md_to_plain_external(content, strip_code_blocks=md_strip_code_blocks)\n",
    "                if verbose:\n",
    "                    print(f\"Markdown converted: {before_len} → {len(content)} chars\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"File size: {len(content)} characters\")\n",
    "\n",
    "            content = content.strip()\n",
    "            texts = [content] if content else []\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Prepared {len(texts)} text(s) for processing\")\n",
    "            if texts and len(texts) == 1:\n",
    "                print(f\"Text length: {len(texts[0])} characters\")\n",
    "            elif texts and len(texts) > 1:\n",
    "                avg_len = sum(len(t) for t in texts) / len(texts)\n",
    "                print(f\"Average text length: {avg_len:.1f} characters\")\n",
    "    else:\n",
    "        texts = [str(text_or_path)]\n",
    "        if verbose:\n",
    "            print(f\"Processing direct text input: {len(texts[0])} characters\")\n",
    "\n",
    "    if not texts:\n",
    "        return {\n",
    "            \"context_length\": context_length,\n",
    "            \"overlap_tokens\": 0 if overlap is None else int(max(0, min(overlap, context_length - 1))),\n",
    "            \"overlap_ratio\": 0.0 if overlap_ratio is None else clamp01(float(overlap_ratio)),\n",
    "            \"num_texts\": 0,\n",
    "            \"PPL_micro\": None,\n",
    "            \"PPL_macro\": None,\n",
    "            \"tokens_evaluated\": 0,\n",
    "            \"used_chat_template\": False,\n",
    "        }\n",
    "\n",
    "    # ---- Compute overlap → step_tokens ----\n",
    "    if overlap is not None:\n",
    "        overlap_tokens = max(0, min(int(overlap), context_length - 1))\n",
    "        effective_overlap_ratio = overlap_tokens / context_length\n",
    "    else:\n",
    "        r = 0.0 if overlap_ratio is None else clamp01(float(overlap_ratio))\n",
    "        overlap_tokens = int(round(context_length * r))\n",
    "        if overlap_tokens >= context_length:\n",
    "            overlap_tokens = context_length - 1\n",
    "        effective_overlap_ratio = overlap_tokens / context_length\n",
    "\n",
    "    step_tokens = max(1, context_length - overlap_tokens)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Context length: {context_length}, Overlap: {overlap_tokens} \"\n",
    "              f\"({effective_overlap_ratio:.2%}), Step: {step_tokens}\")\n",
    "\n",
    "    # ---- Calculate PPL per line ----\n",
    "    ppl_list, tok_list = compute_ppl_per_line(\n",
    "        model_name=model_name,\n",
    "        texts=texts,\n",
    "        batch_size=batch_size,\n",
    "        max_length=context_length,\n",
    "        step_tokens=step_tokens,\n",
    "        use_chat_template=use_chat_template,\n",
    "        eval_role=eval_role,\n",
    "        assistant_user_prompt=assistant_user_prompt,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Token counts per text: {tok_list}\")\n",
    "        print(f\"PPL per text: {[f'{p:.3f}' for p in ppl_list]}\")\n",
    "\n",
    "    # Calculate aggregate metrics\n",
    "    valid_ppls = [p for p in ppl_list if not math.isinf(p)]\n",
    "    total_tokens = sum(tok_list)\n",
    "\n",
    "    if not valid_ppls:\n",
    "        ppl_micro = ppl_macro = None\n",
    "    else:\n",
    "        # Macro: average of individual PPLs\n",
    "        ppl_macro = sum(valid_ppls) / len(valid_ppls)\n",
    "\n",
    "        # Micro: weighted by tokens (approximate)\n",
    "        if total_tokens > 0:\n",
    "            weighted_sum = sum(ppl * tok for ppl, tok in zip(ppl_list, tok_list) if not math.isinf(ppl))\n",
    "            ppl_micro = weighted_sum / total_tokens\n",
    "        else:\n",
    "            ppl_micro = ppl_macro\n",
    "\n",
    "    result = {\n",
    "        \"context_length\": context_length,\n",
    "        \"overlap_tokens\": overlap_tokens,\n",
    "        \"overlap_ratio\": effective_overlap_ratio,\n",
    "        \"num_texts\": len(texts),\n",
    "        \"PPL_micro\": ppl_micro,\n",
    "        \"PPL_macro\": ppl_macro,\n",
    "        \"tokens_evaluated\": total_tokens,\n",
    "        \"used_chat_template\": use_chat_template,\n",
    "    }\n",
    "\n",
    "    # Add per-text results if multiple texts\n",
    "    if len(texts) > 1:\n",
    "        result[\"per_text_ppls\"] = ppl_list\n",
    "        result[\"per_text_tokens\"] = tok_list\n",
    "\n",
    "    return result\n"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_md_content = \"\"\"# โครงการตัวอย่าง: ประเมิน PPL จาก Markdown\n",
    "\n",
    "> บทนำ: เอกสารนี้เขียนด้วย *Markdown* เพื่อทดสอบท่อ PPL\n",
    "\n",
    "## หัวข้อย่อย\n",
    "- รายการที่หนึ่ง: ข้อความภาษาไทยผสมอังกฤษ (Thai + English)\n",
    "- รายการที่สอง: inline code เช่น `tokenize(text)` และลิงก์ [UN](https://www.un.org)\n",
    "\n",
    "ตารางตัวอย่าง (จะถูกแปลงเป็นข้อความล้วน):\n",
    "| คอลัมน์ | ค่า |\n",
    "|---------|-----|\n",
    "| a       | 10  |\n",
    "| b       | 20  |\n",
    "\n",
    "รูปภาพ (จะใช้ alt text): ![โลโก้โครงการ](logo.png)\n",
    "\n",
    "โค้ดบล็อก (fenced code):\n",
    "```python\n",
    "def greet(name):\n",
    "    print(f\"สวัสดี {name}\")\n",
    "greet(\"โลก\")\n",
    "\"\"\"\n",
    "with open(test_file_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้างไฟล์ทดสอบ\n",
    "sample_content = \"\"\"ไอซีอีเอสซีอาร์ (ICESCR) เป็นสนธิสัญญาพหุภาคีซึ่งผ่านมติสมัชชาใหญ่แห่งสหประชาชาติเมื่อวันที่ 16 ธันวาคม ค.ศ. 1966 และมีผลใช้บังคับตั้งแต่วันที่ 3 มกราคม ค.ศ. 1976 เป็นต้นมา กติกาฯ ผูกมัดภาคีให้ทำงานเพื่อมุ่งสู่การให้สิทธิทางเศรษฐกิจ สังคม และการเมือง (อีเอสซีอาร์) แก่ปัจเจกบุคคล รวมถึงสิทธิแรงงานและสิทธิในสุขภาพอนามัย สิทธิในการศึกษา ตลอดจนสิทธิในมาตรฐานการครองชีพที่พอเพียง ณ เดือนกรกฎาคม ค.ศ. 2011 กติกาฯ มีภาคี 160 ประเทศ และยังมีอีก 6 ประเทศที่ได้ลงนามแล้วแต่ยังไม่ได้ให้สัตยาบัน กติกาฯ เป็นส่วนหนึ่งของตราสารสิทธิมนุษยชนระหว่างประเทศร่วมกับปฏิญญาสากลว่าด้วยสิทธิมนุษยชน (ยูดีเอชอาร์) และกติการะหว่างประเทศว่าด้วยสิทธิพลเมืองและสิทธิทางการเมือง (ไอซีซีพีอาร์) และรวมถึงพิธีสารเลือกรับที่หนึ่งและที่สองของกติการะหว่างประเทศว่าด้วยสิทธิพลเมืองและสิทธิทางการเมืองด้วย\"\"\"\n",
    "\n",
    "with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ตั้งค่าโมเดลและข้อมูลทดสอบ\n",
    "model_path = \"/model/gemma-3-270m-it\"\n",
    "base_model_path = \"/model/gemma-3-270m\"  # base model (ถ้ามี)\n",
    "test_file = \"test.txt\"\n",
    "test_file_md = \"test_md.txt\"\n",
    "test_texts = [\n",
    "    \"สวัสดีครับ ผมชื่อจอห์น\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"ประเทศไทยมีความหลากหลายทางวัฒนธรรม\", \n",
    "    \"การประเมิน perplexity ช่วยวัดความสามารถของโมเดล\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. AUTO-DETECT MODE\n",
      "--------------------------------------------------\n"
     ]
    },
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Auto-detect (แนะนำ) - ระบบจะเลือกให้เอง\n",
    "# ==========================================\n",
    "print(\"1. AUTO-DETECT MODE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result1 = compute_ppl(\n",
    "    model_name=model_path,\n",
    "    text_or_path=test_texts,\n",
    "    context_length=32000, #กำหนดตามความเหมาะสม\n",
    "    overlap_ratio=0.25, #อัตราส่วนการเลื่อนของ sliding window กำหนดตามความเหมาะสม \n",
    "    use_chat_template=None,  #None = auto-detect\n",
    "    is_file=False, #บอกว่าไม่ใช่ไฟล์\n",
    "    verbose=True #เพิ่มการพิมพ์ log\n",
    ")\n",
    "\n",
    "print(f\"PPL_micro: {result1.get('PPL_micro', 'N/A'):.4f}\")\n",
    "print(f\"PPL_macro: {result1.get('PPL_macro', 'N/A'):.4f}\")  \n",
    "print(f\"Total tokens: {result1.get('tokens_evaluated', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. BASE MODEL MODE (No Chat Template)\n",
      "--------------------------------------------------\n",
      "PPL_micro: 78.5556\n",
      "PPL_macro: 67.0417\n",
      "Total tokens: 37\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. บังคับใช้เป็น base model (ไม่ใช้ template)\n",
    "# ==========================================\n",
    "print(f\"\\n2. BASE MODEL MODE (No Chat Template)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result2 = compute_ppl(\n",
    "    model_name=base_model_path,  # หรือใช้ base_model_path ถ้ามี\n",
    "    text_or_path=test_texts,\n",
    "    context_length=32000, #กำหนดตามความเหมาะสม\n",
    "    overlap_ratio=0.25, #อัตราส่วนการเลื่อนของ sliding window กำหนดตามความเหมาะสม \n",
    "    use_chat_template=False,  # บังคับไม่ใช้ template\n",
    "    is_file=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"PPL_micro: {result2.get('PPL_micro', 'N/A'):.4f}\")\n",
    "print(f\"PPL_macro: {result2.get('PPL_macro', 'N/A'):.4f}\")\n",
    "print(f\"Total tokens: {result2.get('tokens_evaluated', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. DIRECT TEXT INPUT\n",
      "--------------------------------------------------\n",
      "Processing direct text input: 65 characters\n",
      "Context length: 512, Stride: 409\n",
      "Loading local model from: /model/gemma-3-270m-it\n",
      "Model: /model/gemma-3-270m-it\n",
      "Chat template supported: True\n",
      "Will use chat template: True\n",
      "Processing 1 texts...\n",
      "  Text 1: 65 chars -> 188 chars (after template)\n",
      "  Total tokens: 50\n",
      "  No sliding needed: 1 chunk of 50 tokens\n",
      "Token counts per text: [48]\n",
      "PPL per text: ['618.523']\n",
      "Text: นี่คือการทดสอบ PPL กับข้อความภาษาไทยที่ป้อนเข้าโดยตรง ไม่ผ่านไฟล์\n",
      "PPL_micro: 618.5234\n",
      "PPL_macro: 618.5234\n",
      "Total tokens: 48\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. ใช้ข้อความตรงๆ (ไม่ใช่ไฟล์)\n",
    "# ==========================================\n",
    "print(f\"\\n3. DIRECT TEXT INPUT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "single_text = \"นี่คือการทดสอบ PPL กับข้อความภาษาไทยที่ป้อนเข้าโดยตรง ไม่ผ่านไฟล์\"\n",
    "\n",
    "result3 = compute_ppl(\n",
    "    model_name=model_path,\n",
    "    text_or_path=single_text,\n",
    "    context_length=32000, #กำหนดตามความเหมาะสม\n",
    "    overlap_ratio=0.25, #อัตราส่วนการเลื่อนของ sliding window กำหนดตามความเหมาะสม \n",
    "    is_file=False,  #บอกว่าไม่ใช่ไฟล์\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Text: {single_text}\")\n",
    "print(f\"PPL_micro: {result3.get('PPL_micro', 'N/A'):.4f}\")\n",
    "print(f\"PPL_macro: {result3.get('PPL_macro', 'N/A'):.4f}\")\n",
    "print(f\"Total tokens: {result3.get('tokens_evaluated', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. FILE INPUT\n",
      "--------------------------------------------------\n",
      "Reading file: test.txt\n",
      "File size: 770 characters\n",
      "Prepared 1 text(s) for processing\n",
      "Text length: 770 characters\n",
      "Context length: 2048, Stride: 1638\n",
      "Loading local model from: /model/gemma-3-270m-it\n",
      "Model: /model/gemma-3-270m-it\n",
      "Chat template supported: True\n",
      "Will use chat template: True\n",
      "Processing 1 texts...\n",
      "  Text 1: 771 chars -> 894 chars (after template)\n",
      "  Total tokens: 333\n",
      "  No sliding needed: 1 chunk of 333 tokens\n",
      "Token counts per text: [331]\n",
      "PPL per text: ['36.650']\n",
      "File: test.txt\n",
      "PPL_micro: 36.6495\n",
      "PPL_macro: 36.6495\n",
      "Total tokens: 331\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. ใช้ไฟล์ (txt, markdown, jsonl, csv)\n",
    "# ==========================================\n",
    "print(f\"\\n4. FILE INPUT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result4 = compute_ppl(\n",
    "    model_name=model_path,\n",
    "    text_or_path=test_file,\n",
    "    context_length=32000,\n",
    "    overlap_ratio=0.25,\n",
    "    is_file=True,  # บอกว่าเป็นไฟล์\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"File: {test_file}\")\n",
    "print(f\"PPL_micro: {result4.get('PPL_micro', 'N/A'):.4f}\")\n",
    "print(f\"PPL_macro: {result4.get('PPL_macro', 'N/A'):.4f}\")\n",
    "print(f\"Total tokens: {result4.get('tokens_evaluated', 0)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
